{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1bdd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: D:\\Visual Studio practice\\aviation-chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "os.chdir(r\"D:\\Visual Studio practice\\aviation-chatbot\")\n",
    "\n",
    "print(f\"‚úÖ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5610a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Visual Studio practice\\aviation-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Visual Studio practice\\aviation-chatbot\n",
      "‚úÖ All libraries imported successfully\n",
      "\n",
      "üìÇ PDF Directory: D:\\Visual Studio practice\\aviation-chatbot\\data\\raw_pdfs\n",
      "ü§ñ Embedding Model: all-MiniLM-L6-v2\n",
      "üìè Chunk Size: 400, Overlap: 100\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pdfplumber\n",
    "import json\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "\n",
    "from src.config import (\n",
    "    RAW_PDF_DIR,\n",
    "    DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD,\n",
    "    EMBEDDING_MODEL_NAME, EMBEDDING_BATCH_SIZE,\n",
    "    CHUNK_SIZE, CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"\\nüìÇ PDF Directory: {RAW_PDF_DIR}\")\n",
    "print(f\"ü§ñ Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"üìè Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746324dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 2 PDF files\n",
      "\n",
      "üìÑ Processing: airport_operations.pdf\n",
      "   ‚úÖ Extracted 604 pages\n",
      "üìÑ Processing: scada_manual.pdf\n",
      "   ‚úÖ Extracted 203 pages\n",
      "\n",
      "üéØ Total pages extracted: 807\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF page by page\"\"\"\n",
    "    pages = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text and text.strip():\n",
    "                pages.append({\n",
    "                    \"text\": text.strip(),\n",
    "                    \"page_number\": i + 1,\n",
    "                    \"document_name\": pdf_path.name\n",
    "                })\n",
    "    return pages\n",
    "\n",
    "# Extract from all PDFs\n",
    "all_pages = []\n",
    "pdf_files = list(RAW_PDF_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "print(f\"üìö Found {len(pdf_files)} PDF files\\n\")\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"üìÑ Processing: {pdf_file.name}\")\n",
    "    pages = extract_text_from_pdf(pdf_file)\n",
    "    all_pages.extend(pages)\n",
    "    print(f\"   ‚úÖ Extracted {len(pages)} pages\")\n",
    "\n",
    "print(f\"\\nüéØ Total pages extracted: {len(all_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc71db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Text splitter configuration:\n",
      "   Chunk size: 400\n",
      "   Chunk overlap: 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking pages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 807/807 [00:00<00:00, 14742.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total chunks created: 5011\n",
      "\n",
      "üìä Chunk distribution by document:\n",
      "   airport_operations.pdf: 3825 chunks\n",
      "   scada_manual.pdf: 1186 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(f\"üìù Text splitter configuration:\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"   Chunk overlap: {CHUNK_OVERLAP}\\n\")\n",
    "\n",
    "# Create chunks from all pages\n",
    "all_chunks = []\n",
    "\n",
    "for page in tqdm(all_pages, desc=\"Chunking pages\"):\n",
    "    splits = text_splitter.split_text(page[\"text\"])\n",
    "    \n",
    "    for chunk_text in splits:\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"document_name\": page[\"document_name\"],\n",
    "            \"page_number\": page[\"page_number\"]\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Show distribution\n",
    "doc_counts = Counter(c[\"document_name\"] for c in all_chunks)\n",
    "print(\"\\nüìä Chunk distribution by document:\")\n",
    "for doc, count in doc_counts.items():\n",
    "    print(f\"   {doc}: {count} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bf9f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading embedding model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Visual Studio practice\\aviation-chatbot\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded\n",
      "\n",
      "üßÆ Generating embeddings for 5011 chunks...\n",
      "   Batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [02:12<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Embeddings generated!\n",
      "   Shape: (5011, 384)\n",
      "   Dimension: 384\n",
      "   Data type: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "print(f\"üîÑ Loading embedding model: {EMBEDDING_MODEL_NAME}...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "print(\"‚úÖ Embedding model loaded\\n\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "\n",
    "print(f\"üßÆ Generating embeddings for {len(texts)} chunks...\")\n",
    "print(f\"   Batch size: {EMBEDDING_BATCH_SIZE}\")\n",
    "\n",
    "embeddings = embedding_model.encode(\n",
    "    texts,\n",
    "    batch_size=EMBEDDING_BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Embeddings generated!\")\n",
    "print(f\"   Shape: {embeddings.shape}\")\n",
    "print(f\"   Dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   Data type: {embeddings.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd9f5b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Clearing existing data...\n",
      "‚úÖ Existing data cleared\n",
      "\n",
      "üíæ Inserting 5011 chunks in batches of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [03:55<00:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Successfully inserted 5011 chunks into database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def insert_chunks_to_db(chunks, embeddings, batch_size=100):\n",
    "    \"\"\"\n",
    "    Insert chunks and embeddings into PostgreSQL in batches\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        embeddings: NumPy array of embeddings\n",
    "        batch_size: Number of records to insert at once\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Clear existing data\n",
    "    print(\"üóëÔ∏è  Clearing existing data...\")\n",
    "    cursor.execute(\"DELETE FROM knowledge_chunks;\")\n",
    "    conn.commit()\n",
    "    print(\"‚úÖ Existing data cleared\\n\")\n",
    "    \n",
    "    # Prepare data for batch insert\n",
    "    total_inserted = 0\n",
    "    \n",
    "    print(f\"üíæ Inserting {len(chunks)} chunks in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Inserting batches\"):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        batch_embeddings = embeddings[i:i + batch_size]\n",
    "        \n",
    "        # Prepare batch data\n",
    "        batch_data = [\n",
    "            (\n",
    "                chunk[\"text\"],\n",
    "                embedding.tolist(),  # Convert numpy array to list\n",
    "                chunk[\"document_name\"],\n",
    "                chunk[\"page_number\"]\n",
    "            )\n",
    "            for chunk, embedding in zip(batch_chunks, batch_embeddings)\n",
    "        ]\n",
    "        \n",
    "        # Execute batch insert\n",
    "        cursor.executemany(\"\"\"\n",
    "            INSERT INTO knowledge_chunks (content, embedding, document_name, page_number)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", batch_data)\n",
    "        \n",
    "        conn.commit()\n",
    "        total_inserted += len(batch_data)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return total_inserted\n",
    "\n",
    "# Insert all chunks into database\n",
    "inserted = insert_chunks_to_db(all_chunks, embeddings, batch_size=100)\n",
    "\n",
    "print(f\"\\nüéâ Successfully inserted {inserted} chunks into database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092d1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATABASE VERIFICATION REPORT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Total chunks in database: 5011\n",
      "‚úÖ Unique documents: 2\n",
      "‚úÖ Table size: 18 MB\n",
      "\n",
      "üìö Breakdown by document:\n",
      "   ‚Ä¢ airport_operations.pdf: 3825 chunks\n",
      "   ‚Ä¢ scada_manual.pdf: 1186 chunks\n",
      "\n",
      "üìÑ Sample chunk:\n",
      "   ID: 3\n",
      "   Document: airport_operations.pdf\n",
      "   Page: 2\n",
      "   Content preview:\n",
      "   Airport Operations\n",
      "About the Authors\n",
      "Norman J. Ashford was Professor of Transport Planning at the Loughborough University\n",
      "of Technology, England, from 1972 to 1997. He holds bachelor‚Äôs, master‚Äôs, and doctoral\n",
      "degrees in civil engineering. Dr. Ashford worked as a civil engineer in Canada and taught\n",
      "a...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def verify_database():\n",
    "    \"\"\"Verify data was inserted correctly\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Total count\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM knowledge_chunks;\")\n",
    "    total = cursor.fetchone()[0]\n",
    "    \n",
    "    # Unique documents\n",
    "    cursor.execute(\"SELECT COUNT(DISTINCT document_name) FROM knowledge_chunks;\")\n",
    "    unique_docs = cursor.fetchone()[0]\n",
    "    \n",
    "    # Documents breakdown\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT document_name, COUNT(*) as chunk_count\n",
    "        FROM knowledge_chunks\n",
    "        GROUP BY document_name\n",
    "        ORDER BY chunk_count DESC;\n",
    "    \"\"\")\n",
    "    doc_breakdown = cursor.fetchall()\n",
    "    \n",
    "    # Table size\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT pg_size_pretty(pg_total_relation_size('knowledge_chunks'));\n",
    "    \"\"\")\n",
    "    table_size = cursor.fetchone()[0]\n",
    "    \n",
    "    # Sample chunk\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT id, content, document_name, page_number\n",
    "        FROM knowledge_chunks\n",
    "        LIMIT 1;\n",
    "    \"\"\")\n",
    "    sample = cursor.fetchone()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìä DATABASE VERIFICATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n‚úÖ Total chunks in database: {total}\")\n",
    "    print(f\"‚úÖ Unique documents: {unique_docs}\")\n",
    "    print(f\"‚úÖ Table size: {table_size}\")\n",
    "    \n",
    "    print(\"\\nüìö Breakdown by document:\")\n",
    "    for doc_name, count in doc_breakdown:\n",
    "        print(f\"   ‚Ä¢ {doc_name}: {count} chunks\")\n",
    "    \n",
    "    print(\"\\nüìÑ Sample chunk:\")\n",
    "    print(f\"   ID: {sample[0]}\")\n",
    "    print(f\"   Document: {sample[2]}\")\n",
    "    print(f\"   Page: {sample[3]}\")\n",
    "    print(f\"   Content preview:\\n   {sample[1][:300]}...\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "verify_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303ec984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing similarity search...\n",
      "   Query: 'What is SCADA and how does it work?'\n",
      "   Retrieving top 3 results\n",
      "\n",
      "================================================================================\n",
      "üéØ SEARCH RESULTS\n",
      "================================================================================\n",
      "\n",
      "1. [scada_manual.pdf | Page 8] (Similarity: 0.6833)\n",
      "   Preface to the Third Edition\n",
      "When the first edition of this book was written, certain trends in SCADA\n",
      "were already apparent, and I made attempts to identify them in Unit 14,\n",
      "\"What's Next?\". Generally, these trends have continued, and their\n",
      "descriptio...\n",
      "\n",
      "2. [scada_manual.pdf | Page 13] (Similarity: 0.6543)\n",
      "   electronics-based technologies, SCAD A is a virtual cornucopia of these\n",
      "terms and abbreviations. Finally, the solutions to the exercises found at the\n",
      "end of each unit are given in Appendix C.\n",
      "1-6. Course Objectives\n",
      "When you have completed this entire...\n",
      "\n",
      "3. [scada_manual.pdf | Page 145] (Similarity: 0.6444)\n",
      "   respectively. Very often people confuse the terms in the mistaken belief\n",
      "that because SCAD A is associated with so much expensive computer\n",
      "hardware it must be automatic. In fact, most early SCADA systems and a\n",
      "large minority of present ones are not a...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def test_similarity_search(query_text, top_k=5):\n",
    "    \"\"\"Test vector similarity search\"\"\"\n",
    "    print(f\"üîç Testing similarity search...\")\n",
    "    print(f\"   Query: '{query_text}'\")\n",
    "    print(f\"   Retrieving top {top_k} results\\n\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query_text])[0]\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Perform similarity search using cosine distance\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            content,\n",
    "            document_name,\n",
    "            page_number,\n",
    "            1 - (embedding <=> %s::vector) as similarity\n",
    "        FROM knowledge_chunks\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s;\n",
    "    \"\"\", (query_embedding.tolist(), query_embedding.tolist(), top_k))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéØ SEARCH RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, (chunk_id, content, doc_name, page_num, similarity) in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. [{doc_name} | Page {page_num}] (Similarity: {similarity:.4f})\")\n",
    "        print(f\"   {content[:250]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Test with aviation-related queries\n",
    "test_similarity_search(\"What is SCADA and how does it work?\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afef03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
