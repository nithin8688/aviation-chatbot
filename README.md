# ‚úàÔ∏è Aviation Chatbot - RAG-Based Technical Documentation Assistant

A production-ready, AI-powered chatbot for querying aviation technical documents using Retrieval-Augmented Generation (RAG) with PostgreSQL vector database and Google Gemini LLM.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31.0-red.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

üîó **Live Demo:** [Add your deployed URL here]

---

## üìã Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Performance](#performance)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [How It Works](#how-it-works)
- [Configuration](#configuration)
- [Deployment](#deployment)
- [Development](#development)
- [Troubleshooting](#troubleshooting)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [License](#license)

---

## üéØ Overview

The Aviation Chatbot is an intelligent question-answering system designed for aviation professionals, students, and enthusiasts. It provides accurate, citation-backed answers from technical documentation including:

- Airport Operations manuals
- SCADA Systems documentation
- Aviation safety regulations (Gazette of India)
- Aircraft handling procedures
- Airport engineering and design
- Baggage handling systems
- Night operations procedures
- And more...

**Key Innovation:** Unlike simple keyword search, this system uses **hybrid search** (BM25 + vector embeddings) and generates contextual answers using large language models.

---

## ‚ú® Features

### Core Capabilities

- **üîç Hybrid Search**: Combines BM25 keyword search with vector embeddings for superior accuracy
- **ü§ñ AI-Powered Answers**: Context-aware responses generated by Google Gemini 2.5 Flash
- **üì§ Dynamic PDF Upload**: Users can upload new documents without restarting the system
- **üìö Source Citations**: Every answer includes references to source documents and page numbers
- **üîí Privacy Protection**: Prevents leakage of internal document details for off-topic queries
- **‚ö° Real-time Processing**: Live progress tracking for PDF ingestion
- **üíæ Persistent Storage**: PostgreSQL database for reliable, scalable data management
- **üìä Table Extraction**: Automatically extracts and formats tables from PDFs as Markdown

### User Experience

- **üí¨ Chat Interface**: Clean, modern Streamlit UI similar to ChatGPT
- **üìä System Monitoring**: Real-time database statistics and document management
- **üé® Dark Theme**: Eye-friendly interface with high contrast
- **üì± Responsive Design**: Works on desktop and tablet devices
- **üóëÔ∏è Document Management**: Delete and re-upload documents as needed

---

## üìä Performance

### Production Metrics (Optimized)

| Metric | Value |
|--------|-------|
| **Total Documents** | 8 PDFs |
| **Total Chunks** | 7,280 chunks |
| **Average Response Time** | 5.9 seconds |
| **Simple Queries** | 3-5 seconds |
| **Complex Queries** | 5-7 seconds |
| **Test Success Rate** | 100% ‚úÖ |
| **Embedding Time** | ~40ms |
| **Hybrid Search Time** | ~80ms (cached) |
| **Gemini API Time** | 3-5 seconds |

### Optimized Configuration

```python
TOP_K_RETRIEVAL = 5         # Optimized from 8 for speed
USE_HYBRID_SEARCH = True    # BM25 + Vector fusion (cached)
USE_HYDE = False            # Disabled for speed
USE_RERANKING = False       # Disabled for speed
HYBRID_ALPHA = 0.6          # 60% vector, 40% BM25
```

**Performance improvements:**
- ‚úÖ 4√ó faster than initial (20s ‚Üí 5s)
- ‚úÖ BM25 index cached (100√ó faster on repeat queries)
- ‚úÖ Connection pooling
- ‚úÖ Bulk inserts (20√ó faster ingestion)

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     User Interface (Streamlit)               ‚îÇ
‚îÇ  ‚Ä¢ Chat Input/Output  ‚Ä¢ PDF Upload  ‚Ä¢ Settings  ‚Ä¢ Stats     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      RAG Engine (Python)                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ   Retrieval  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Generation  ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ   (Hybrid:   ‚îÇ         ‚îÇ  (Gemini     ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ   BM25 +     ‚îÇ         ‚îÇ   LLM)       ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ   Vector)    ‚îÇ         ‚îÇ              ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                      ‚îÇ
               ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL + pgvector ‚îÇ  ‚îÇ  Google Gemini API   ‚îÇ
‚îÇ  ‚Ä¢ Document Chunks     ‚îÇ  ‚îÇ  ‚Ä¢ Answer Generation ‚îÇ
‚îÇ  ‚Ä¢ Vector Embeddings   ‚îÇ  ‚îÇ  ‚Ä¢ Context Synthesis ‚îÇ
‚îÇ  ‚Ä¢ BM25 Index (cached) ‚îÇ  ‚îÇ                      ‚îÇ
‚îÇ  ‚Ä¢ Metadata            ‚îÇ  ‚îÇ                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚ñ≤
               ‚îÇ
               ‚îÇ (Ingestion Pipeline)
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PDF Processing Pipeline (Enhanced)             ‚îÇ
‚îÇ  PDF ‚Üí Extract ‚Üí Tables ‚Üí Chunk ‚Üí Embed ‚Üí Store            ‚îÇ
‚îÇ  (pdfplumber ‚Üí Markdown ‚Üí RecursiveTextSplitter ‚Üí          ‚îÇ
‚îÇ   Sentence Transformers ‚Üí PostgreSQL)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

### Backend
- **Python 3.11+**: Core programming language
- **PostgreSQL 16**: Primary database with vector extension
- **pgvector 0.8.1**: Vector similarity search extension
- **Streamlit 1.31.0**: Web application framework

### AI/ML
- **Google Gemini 2.5 Flash**: Large language model for answer generation
- **Sentence Transformers**: Text embedding model (all-MiniLM-L6-v2, 384-d)
- **Hugging Face Transformers**: Model loading and tokenization
- **rank-bm25**: BM25 keyword search algorithm

### Data Processing
- **LangChain**: Text splitting and chunking utilities
- **pdfplumber**: PDF text and table extraction
- **NumPy**: Data manipulation

### Infrastructure
- **Docker**: Containerized PostgreSQL deployment
- **psycopg2**: PostgreSQL database adapter with connection pooling

---

## üì• Installation

### Prerequisites

1. **Python 3.11 or higher**
   ```bash
   python --version
   ```

2. **Docker Desktop** (for PostgreSQL)
   - Download: https://www.docker.com/products/docker-desktop

3. **Gemini API Key** (free)
   - Get it from: https://ai.google.dev/

### Step-by-Step Setup

#### 1. Clone the Repository

```bash
git clone https://github.com/nithin8688/aviation-chatbot.git
cd aviation-chatbot
```

#### 2. Create Virtual Environment

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Mac/Linux
source .venv/bin/activate
```

#### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

#### 4. Start PostgreSQL Container

```bash
docker run --name aviation-postgres \
  -e POSTGRES_PASSWORD=aviation123 \
  -e POSTGRES_DB=aviation_chatbot \
  -p 5432:5432 \
  -d pgvector/pgvector:pg16
```

#### 5. Configure Environment

Create `.env` file in project root:

```bash
# .env file
DB_HOST=localhost
DB_PORT=5432
DB_NAME=aviation_chatbot
DB_USER=postgres
DB_PASSWORD=aviation123
GEMINI_API_KEY=your-api-key-here
```

**‚ö†Ô∏è IMPORTANT:** Never commit `.env` to Git! It's in `.gitignore`.

#### 6. Initialize Database

The database table is created automatically on first run. Or run manually:

```python
# Python console
from src.db_utils import *
from src.config import KNOWLEDGE_CHUNKS_TABLE
import psycopg2

conn = psycopg2.connect(
    host="localhost",
    port=5432,
    database="aviation_chatbot",
    user="postgres",
    password="aviation123"
)
conn.autocommit = True
cursor = conn.cursor()
cursor.execute(KNOWLEDGE_CHUNKS_TABLE)
cursor.close()
conn.close()
```

#### 7. Launch the Application

```bash
streamlit run app.py
```

The app will open at `http://localhost:8501`

---

## üöÄ Usage

### Asking Questions

1. **Start the App**: `streamlit run app.py`
2. **Type Your Question**: Use the chat input at the bottom
3. **View Answer**: Response appears with source citations
4. **Check Sources**: Expand "View Sources" to see document excerpts

**Example Questions:**
- "What is baggage?" ‚Üí Gets detailed answer from baggage handling PDF
- "What is the gazette of india?" ‚Üí Explains what the document contains
- "What is ILS?" ‚Üí Technical definition from airport operations manual
- "What are the responsibilities of airport apron control?"
- "Explain runway safety procedures"

### Uploading New Documents

1. **Click**: "üì§ Upload New PDF" in the sidebar
2. **Select**: Choose a PDF file (max 200MB)
3. **Process**: Upload starts automatically
4. **Monitor**: Watch real-time progress bar
5. **Query**: Ask questions about the new document immediately

### Managing Documents

1. **View Stats**: Check "üìä Knowledge Base" for database info
2. **Delete Documents**: Use "üóëÔ∏è Documents in Database" section
3. **Re-upload**: Delete and upload new version if needed

---

## üìÅ Project Structure

```
aviation-chatbot/
‚îÇ
‚îú‚îÄ‚îÄ app.py                          # Main Streamlit application
‚îÇ
‚îú‚îÄ‚îÄ src/                            # Core application code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Configuration & optimized settings
‚îÇ   ‚îú‚îÄ‚îÄ db_utils.py                 # Database ops with connection pooling
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine.py               # RAG pipeline with timing
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py                   # PDF ingestion with table extraction
‚îÇ   ‚îî‚îÄ‚îÄ hybrid_search.py            # BM25 + vector search (cached)
‚îÇ
‚îú‚îÄ‚îÄ data/                           # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ raw_pdfs/                   # Initial PDF documents
‚îÇ   ‚îî‚îÄ‚îÄ uploaded_pdfs/              # User-uploaded PDFs
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ .env                            # Environment variables (NOT in git)
‚îú‚îÄ‚îÄ .gitignore                      # Git ignore rules
‚îú‚îÄ‚îÄ README.md                       # This file
‚îî‚îÄ‚îÄ DEPLOYMENT_GUIDE.md             # Complete deployment instructions
```

**Uploaded Documents:**
1. Airport Engineering Planning Design and Development.pdf (3,026 chunks)
2. airport_operations.pdf (1,915 chunks)
3. computerized maintenance management systems.pdf (829 chunks)
4. Airport Baggage Handling Systems Using the Baggage.pdf (713 chunks)
5. scada_manual.pdf (608 chunks)
6. THE GAZETTE OF INDIA EXTRAORDINARY.pdf (111 chunks)
7. Night operations.pdf (71 chunks)
8. Annual Appraisal Policypdf.pdf (7 chunks)

---

## üîÑ How It Works

### 1. Document Ingestion Pipeline

```python
PDF Document
    ‚Üì
[Extract Text & Tables] (pdfplumber)
    ‚îú‚îÄ Text content
    ‚îú‚îÄ Tables ‚Üí Markdown format
    ‚îî‚îÄ Page metadata
    ‚Üì
[Chunk Text] (RecursiveCharacterTextSplitter)
    ‚îú‚îÄ Size: 800 chars
    ‚îú‚îÄ Overlap: 200 chars
    ‚îî‚îÄ Preserve semantic boundaries
    ‚Üì
[Generate Embeddings] (all-MiniLM-L6-v2)
    ‚îú‚îÄ Batch size: 64
    ‚îú‚îÄ Dimension: 384
    ‚îî‚îÄ Process-level caching
    ‚Üì
[Store in PostgreSQL]
    ‚îú‚îÄ Content: Text chunk
    ‚îú‚îÄ Embedding: vector(384)
    ‚îú‚îÄ Metadata: {doc_name, page_number}
    ‚îî‚îÄ Index: IVFFlat for fast search
```

### 2. Query Processing Pipeline (Optimized)

```python
User Question
    ‚Üì
[Generate Query Embedding] (all-MiniLM-L6-v2, ~40ms)
    ‚Üì
[Hybrid Search] (BM25 + Vector, ~80ms cached)
    ‚îú‚îÄ BM25: Keyword matching (cached index)
    ‚îú‚îÄ Vector: Semantic similarity (pgvector)
    ‚îî‚îÄ Fusion: 60% vector + 40% BM25
    ‚Üì
[Retrieve Top K Chunks] (K=5, optimized for speed)
    ‚îú‚îÄ Ranked by combined score
    ‚îî‚îÄ Source metadata included
    ‚Üì
[Build Context] (RAG Engine)
    ‚îú‚îÄ Combine retrieved chunks
    ‚îî‚îÄ Add source citations
    ‚Üì
[Generate Answer] (Gemini 2.5 Flash, ~3-5s)
    ‚îú‚îÄ System prompt with instructions
    ‚îú‚îÄ Context from retrieved docs
    ‚îú‚îÄ User query
    ‚îî‚îÄ Privacy protection for off-topic queries
    ‚Üì
[Format Response]
    ‚îú‚îÄ Answer text
    ‚îú‚îÄ Source citations (doc + page)
    ‚îî‚îÄ Relevance scores
    ‚Üì
Display to User
```

### 3. Key Algorithms

**Chunking Strategy:**
- **Method**: Recursive Character Text Splitter
- **Size**: 800 characters (optimized for technical docs)
- **Overlap**: 200 characters (maintains context continuity)
- **Separators**: `["\n\n", "\n", ". ", " ", ""]`

**Embedding Model:**
- **Model**: all-MiniLM-L6-v2
- **Dimension**: 384
- **Speed**: ~32 sentences/second on CPU
- **Quality**: Good balance of speed and accuracy

**Hybrid Search (NEW):**
- **BM25**: Exact keyword matching (great for acronyms, IDs, part numbers)
- **Vector**: Semantic similarity (understands meaning)
- **Fusion**: HYBRID_ALPHA = 0.6 (60% vector + 40% BM25)
- **Caching**: BM25 index cached, rebuilt only when documents change

**Vector Search:**
- **Index**: IVFFlat (Inverted File with Flat compression)
- **Distance**: Cosine similarity (1 - cosine distance)
- **Lists**: 100 (for index optimization)

**Answer Generation:**
- **Model**: Google Gemini 2.5 Flash
- **Temperature**: 0.1 (deterministic answers)
- **Max Tokens**: Adaptive based on context
- **Rate Limit**: 15 requests/minute (free tier)

---

## üîß Configuration

### Key Settings (`src/config.py`)

```python
# Performance Optimized Settings
TOP_K_RETRIEVAL = 5           # Number of chunks (reduced from 8 for speed)
SIMILARITY_THRESHOLD = 0.65   # Minimum similarity score

# Hybrid Search (NEW)
USE_HYBRID_SEARCH = True      # Enable BM25 + vector fusion
HYBRID_ALPHA = 0.6            # 60% vector, 40% BM25

# Speed Optimizations
USE_HYDE = False              # Disabled (saves 2s + API call)
USE_RERANKING = False         # Disabled (saves 500ms)

# Chunking
CHUNK_SIZE = 800              # Chars per chunk
CHUNK_OVERLAP = 200           # Overlap between chunks

# Embeddings
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_BATCH_SIZE = 64     # Faster processing

# LLM
GEMINI_MODEL = "gemini-2.5-flash"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Database
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = int(os.getenv("DB_PORT", "5432"))
DB_NAME = os.getenv("DB_NAME", "aviation_chatbot")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD")
```

### Environment Variables

Create `.env` file:
```bash
# Database
DB_HOST=localhost
DB_PORT=5432
DB_NAME=aviation_chatbot
DB_USER=postgres
DB_PASSWORD=aviation123

# Gemini API
GEMINI_API_KEY=your_api_key_here
```

---

## üåê Deployment

### Quick Deploy to Streamlit Cloud (Recommended)

**See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for complete step-by-step instructions!**

**Summary:**
1. Push code to GitHub
2. Create cloud PostgreSQL database (Render.com)
3. Connect to Streamlit Cloud
4. Add secrets (database credentials + API key)
5. Deploy!

**Deployed on:**
- ‚òÅÔ∏è **Streamlit Cloud** - Frontend (Free tier)
- üóÑÔ∏è **Render.com PostgreSQL** - Database (Free tier)

### Alternative: Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

```bash
# Build and run
docker build -t aviation-chatbot .
docker run -p 8501:8501 --env-file .env aviation-chatbot
```

### Cloud Deployment Options

1. **Streamlit Cloud** (Easiest) ‚≠ê
   - Connect GitHub repo
   - Add secrets in dashboard
   - Deploy with one click
   - **FREE!**

2. **Render.com**
   - Web service + PostgreSQL
   - Auto-deploys from GitHub
   - Free tier available

3. **Heroku**
   ```bash
   heroku create aviation-chatbot
   heroku addons:create heroku-postgresql
   git push heroku main
   ```

4. **AWS/GCP/Azure**
   - Deploy with container service
   - Use managed PostgreSQL
   - Set environment variables

---

## üë®‚Äçüíª Development

### Running Tests

```bash
# Test database connection
python -c "from src.db_utils import get_total_chunks; print(get_total_chunks())"

# Test RAG engine
python -c "from src.rag_engine import get_rag_engine; engine = get_rag_engine(); print('OK')"

# Run diagnostic
python diagnostic.py

# Run quick tests
python quick_test.py
```

**Expected output from quick_test.py:**
```
‚úÖ All tests passed (100% success)
‚úÖ Average time: 5.91s
‚úÖ READY FOR PRODUCTION!
```

### Adding New Features

1. **Fork** the repository
2. **Create** feature branch: `git checkout -b feature/new-feature`
3. **Commit** changes: `git commit -m 'Add new feature'`
4. **Push** to branch: `git push origin feature/new-feature`
5. **Open** Pull Request

---

## üêõ Troubleshooting

### Common Issues

**1. Docker Container Not Running**
```bash
docker start aviation-postgres
docker ps  # Verify it's running
```

**2. API Key Error**
```bash
# Check if key is set
echo $GEMINI_API_KEY

# Or set in .env file
GEMINI_API_KEY=your-key-here
```

**3. "DB_PASSWORD is not set"**
```bash
# Create .env file with:
DB_PASSWORD=aviation123
GEMINI_API_KEY=your-key-here
```

**4. Database Connection Failed**
```bash
# Check PostgreSQL is running
docker logs aviation-postgres

# Verify credentials in .env
DB_PASSWORD=aviation123
```

**5. Slow Responses (> 10s)**
```python
# In src/config.py, reduce TOP_K:
TOP_K_RETRIEVAL = 3  # Even faster (was 5)
```

**6. BM25 Cache Not Working**
```bash
# Check console output - should see this ONCE:
üì¶ Building BM25 index (7280 chunks)...
‚úÖ BM25 index ready

# Subsequent queries should NOT rebuild
```

**7. Import Errors**
```bash
pip install --upgrade sentence-transformers
pip install --upgrade google-genai
pip install rank-bm25
```

---

## üöÄ Future Enhancements

### Phase 5 Features (Optional)

- [ ] **Conversation Memory**: Multi-turn dialogues with context
- [ ] **Semantic Chunking**: Smart chunk boundaries based on meaning
- [ ] **Production Logging**: Analytics and monitoring dashboard
- [ ] **API Wrapper**: RESTful API for integrations
- [ ] **Async Processing**: psycopg3 for non-blocking queries
- [ ] **Redis Caching**: Cache query results for faster responses

### Long-term Roadmap

- [ ] **Multi-language Support**: Translate documents and queries
- [ ] **Advanced Filters**: Search by date, author, document type
- [ ] **Export Conversations**: Download chat history as PDF
- [ ] **Voice Input**: Speech-to-text for hands-free queries
- [ ] **Collaborative Features**: Share queries and answers with team
- [ ] **Fine-tuned Model**: Custom embeddings for aviation terminology
- [ ] **Mobile App**: Native iOS/Android applications

### Performance Optimizations (Completed ‚úÖ)

- [x] **Hybrid Search**: BM25 + vector for better accuracy
- [x] **BM25 Caching**: 100√ó faster repeat queries
- [x] **Connection Pooling**: Prevent connection leaks
- [x] **Bulk Inserts**: 20√ó faster ingestion
- [x] **Table Extraction**: Markdown formatting for tables

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Guidelines

1. Follow PEP 8 style guide
2. Add tests for new features
3. Update documentation
4. Keep commits atomic and well-described

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üëè Acknowledgments

- **Google Gemini** for powerful LLM capabilities
- **Sentence Transformers** for efficient embeddings
- **PostgreSQL + pgvector** for vector database functionality
- **Streamlit** for rapid UI development
- **LangChain** for text processing utilities
- **rank-bm25** for keyword search capabilities

---

## üìû Contact

**Project Maintainer**: Nithin
- GitHub: [@nithin8688](https://github.com/nithin8688)
- Email: nithin.dev8688@gmail.com

**Project Link**: https://github.com/nithin8688/aviation-chatbot

---

## üìä Project Stats

- **Total Lines of Code**: ~3,000
- **Documents Indexed**: 8 PDFs (7,280 chunks)
- **Average Query Time**: 5.9 seconds (optimized)
- **Test Success Rate**: 100%
- **Supported PDF Size**: Up to 200MB per file
- **Database Size**: ~1.5MB per 1,000 chunks
- **Embedding Dimension**: 384
- **Total Embeddings**: 2.8M parameters (7,280 √ó 384)

---

**Built with ‚ù§Ô∏è for the Aviation Community**

*Production-ready RAG system with hybrid search, optimized performance, and comprehensive documentation.*

*Last Updated: February 2026*